{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0908d8-a6b9-4b64-a364-d9bba44ac9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/12 20:14:25 WARN Utils: Your hostname, irtaza-Victus-by-HP-Gaming-Laptop-15-fa0xxx resolves to a loopback address: 127.0.1.1; using 2407:d000:d:7347:158e:b138:1774:d316 instead (on interface wlp4s0)\n",
      "24/05/12 20:14:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/05/12 20:14:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Ensure the path to the JAR files are correct and accessible\n",
    "path_to_mongo_spark_connector = \"/home/hdoop/Downloads/mongo-spark-connector_2.12-3.0.2.jar\"\n",
    "path_to_mongo_java_driver = \"/home/hdoop/Downloads/mongo-java-driver-3.12.10.jar\"\n",
    "\n",
    "print(\"Creating SparkSession...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MusicRecommendation\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/music_recommendation.audio_features\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/music_recommendation.audio_features\") \\\n",
    "    .config(\"spark.jars\", \",\".join([path_to_mongo_spark_connector, path_to_mongo_java_driver])) \\\n",
    "    .getOrCreate()\n",
    "print(\"SparkSession created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c5d03c-dccf-465f-8bb8-e8024aa891e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed audio features from MongoDB into a Spark DataFrame\n",
    "print(\"Loading data from MongoDB...\")\n",
    "audio_features = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1:27017/music_recommendation.audio_features\").load()\n",
    "print(\"Data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a669e0b-5e11-4827-bc8b-566aa0fedf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the loaded data: 104478 rows x 2 columns\n",
      "Columns in the DataFrame:\n",
      "_id\n",
      "features\n",
      "First 50 rows of the DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 _id|            features|\n",
      "+--------------------+--------------------+\n",
      "|{663cfc46eff9941d...|[[3.6756520370513...|\n",
      "|{663cfc46eff9941d...|[[-4.059843640725...|\n",
      "|{663cfc47eff9941d...|[[-0.684101795088...|\n",
      "|{663cfc47eff9941d...|[[-1.278270721028...|\n",
      "|{663cfc47eff9941d...|[[-1.447105094707...|\n",
      "|{663cfc47eff9941d...|[[-2.751079754458...|\n",
      "|{663cfc47eff9941d...|[[-1.501083218520...|\n",
      "|{663cfc48eff9941d...|[[1.2266289928350...|\n",
      "|{663cfc48eff9941d...|[[-2.031567618177...|\n",
      "|{663cfc48eff9941d...|[[-1.707836872826...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the data\n",
    "print(\"Size of the loaded data: {} rows x {} columns\".format(audio_features.count(), len(audio_features.columns)))\n",
    "\n",
    "# Print the columns\n",
    "print(\"Columns in the DataFrame:\")\n",
    "for col_name in audio_features.columns:\n",
    "      print(col_name)\n",
    "\n",
    "# Show the first 50 rows\n",
    "print(\"First 50 rows of the DataFrame:\")\n",
    "audio_features.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb5b362d-97ab-4631-a79b-280c73da0bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into training and testing sets...\n",
      "Data splitting completed.\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "print(\"Splitting data into training and testing sets...\")\n",
    "# Define the ratio for splitting (e.g., 80% training, 20% testing)\n",
    "training_ratio = 0.8\n",
    "testing_ratio = 1.0 - training_ratio\n",
    "\n",
    "# Split the data\n",
    "training_data, test_data = audio_features.randomSplit([training_ratio, testing_ratio], seed=1234)\n",
    "\n",
    "# Indicate when data splitting is done\n",
    "print(\"Data splitting completed.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "729fe55c-d85f-4f5a-930f-64ac9794d999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:======================================================>(575 + 4) / 579]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the loaded data: 83284 rows x 2 columns\n",
      "Columns in the DataFrame:\n",
      "_id\n",
      "features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print the size of the data\n",
    "print(\"Size of the loaded data: {} rows x {} columns\".format(training_data.count(), len(training_data.columns)))\n",
    "\n",
    "# Print the columns\n",
    "print(\"Columns in the DataFrame:\")\n",
    "for col_name in training_data.columns:\n",
    "    print(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc80a029-7d26-4f71-b2e1-f50a81ab4bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:======================================================>(578 + 1) / 579]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the loaded data: 20965 rows x 2 columns\n",
      "Columns in the DataFrame:\n",
      "_id\n",
      "features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print the size of the data\n",
    "print(\"Size of the loaded data: {} rows x {} columns\".format(test_data.count(), len(test_data.columns)))\n",
    "\n",
    "# Print the columns\n",
    "print(\"Columns in the DataFrame:\")\n",
    "for col_name in test_data.columns:\n",
    "   print(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a91d10e7-3902-436b-9224-019a1c563f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a1d2363-19bc-4c2d-a2e0-bbc51f72af5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- user_id: double (nullable = true)\n",
      " |-- track_id: double (nullable = true)\n",
      " |-- play_count: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|              row_id|            user_id|           track_id|         play_count|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|{663cfc46eff9941d...| 3.6756520370513175| 3.1075330221641035|  2.849467203982779|\n",
      "|{663cfc47eff9941d...|-0.6841017950883698|0.07530321083923087| 0.7578103674944087|\n",
      "|{663cfc47eff9941d...|-1.2782707210287152| -1.404545350553354|-0.9536318207970136|\n",
      "|{663cfc47eff9941d...|-2.7510797544587415|-1.2052667907857624|-0.8085763432086499|\n",
      "|{663cfc47eff9941d...| -1.501083218520508|-1.3162265608271722| -1.176454029434011|\n",
      "|{663cfc48eff9941d...| 1.2266289928350496|  3.156820016138062| 3.7090223564897933|\n",
      "|{663cfc48eff9941d...| -2.031567618177572|-0.3587792300901715| 0.6069387849694297|\n",
      "|{663cfc48eff9941d...| -1.707836872826653| -1.230572212985931| -1.309623153998263|\n",
      "|{663cfc48eff9941d...|  1.307634124710383| 1.7670893955220124|  1.068281944658892|\n",
      "|{663cfc49eff9941d...|  2.020557111559502|  3.790770968128053|  4.381158322343134|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Split 'features' array into separate columns\n",
    "split_features = training_data.select(\n",
    "    col(\"_id\").alias(\"row_id\"),  # Assuming \"_id\" is the correct column name\n",
    "    col(\"features\")[0][0].alias(\"user_id\"),\n",
    "    col(\"features\")[0][1].alias(\"track_id\"),\n",
    "    col(\"features\")[0][2].alias(\"play_count\")\n",
    ")\n",
    "\n",
    "# Show the schema and a sample of the split data\n",
    "split_features.printSchema()\n",
    "split_features.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "076d459c-fa4c-4f09-ae82-25e6a8b63e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- user_id: double (nullable = true)\n",
      " |-- track_id: double (nullable = true)\n",
      " |-- play_count: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|              row_id|            user_id|           track_id|         play_count|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|{663cfc46eff9941d...| 3.6756520370513175| 3.1075330221641035|  2.849467203982779|\n",
      "|{663cfc47eff9941d...|-0.6841017950883698|0.07530321083923087| 0.7578103674944087|\n",
      "|{663cfc47eff9941d...|-1.2782707210287152| -1.404545350553354|-0.9536318207970136|\n",
      "|{663cfc47eff9941d...|-2.7510797544587415|-1.2052667907857624|-0.8085763432086499|\n",
      "|{663cfc47eff9941d...| -1.501083218520508|-1.3162265608271722| -1.176454029434011|\n",
      "|{663cfc48eff9941d...| 1.2266289928350496|  3.156820016138062| 3.7090223564897933|\n",
      "|{663cfc48eff9941d...| -2.031567618177572|-0.3587792300901715| 0.6069387849694297|\n",
      "|{663cfc48eff9941d...| -1.707836872826653| -1.230572212985931| -1.309623153998263|\n",
      "|{663cfc48eff9941d...|  1.307634124710383| 1.7670893955220124|  1.068281944658892|\n",
      "|{663cfc49eff9941d...|  2.020557111559502|  3.790770968128053|  4.381158322343134|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Split 'features' array into separate columns\n",
    "split_features01 = test_data.select(\n",
    "    col(\"_id\").alias(\"row_id\"),  # Assuming \"_id\" is the correct column name\n",
    "    col(\"features\")[0][0].alias(\"user_id\"),\n",
    "    col(\"features\")[0][1].alias(\"track_id\"),\n",
    "    col(\"features\")[0][2].alias(\"play_count\")\n",
    ")\n",
    "\n",
    "# Show the schema and a sample of the split data\n",
    "split_features.printSchema()\n",
    "split_features.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6354a820-2ddf-4de8-aa80-a27c5c873db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/12 19:29:39 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+------------------+-------------------+\n",
      "|summary|              row_id|            user_id|          track_id|         play_count|\n",
      "+-------+--------------------+-------------------+------------------+-------------------+\n",
      "|  count|               83330|              83330|             83330|              83330|\n",
      "|   mean|2.462881038278719...|0.17712708508340333|0.2483019320772831|0.31296051842073686|\n",
      "| stddev|1.414356045106426...|  2.518530739645346| 2.142208030876912|  2.094931943083896|\n",
      "|    min|                   0|                -12|                -8|                 -7|\n",
      "|    25%|       1245540515939|                 -1|                -1|                 -1|\n",
      "|    50%|       2448131358755|                  0|                 0|                  0|\n",
      "|    75%|       3667902070811|                  1|                 1|                  1|\n",
      "|    max|       4964982194190|                 58|                44|                 34|\n",
      "+-------+--------------------+-------------------+------------------+-------------------+\n",
      "\n",
      "Data transformation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split, monotonically_increasing_id\n",
    "\n",
    "# Add a unique identifier column to the DataFrame\n",
    "training_data = training_data.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "# Split the 'features' column into separate columns for user_id, track_id, and play_count\n",
    "training_data = training_data.withColumn(\"user_id\", split(col(\"features\")[0][0].cast(\"string\"), \" \")[0].cast(\"int\"))\n",
    "training_data = training_data.withColumn(\"track_id\", split(col(\"features\")[0][1].cast(\"string\"), \" \")[0].cast(\"int\"))\n",
    "training_data = training_data.withColumn(\"play_count\", split(col(\"features\")[0][2].cast(\"string\"), \" \")[0].cast(\"int\"))\n",
    "\n",
    "# Drop rows with null values\n",
    "training_data = training_data.dropna()\n",
    "\n",
    "# Show summary of cleaned data\n",
    "training_data.summary().show()\n",
    "\n",
    "# Indicate when data transformation is done\n",
    "print(\"Data transformation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "777c62a0-6b9b-4322-a65a-924c68e17e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+-------------------+------------------+\n",
      "|summary|              row_id|           user_id|           track_id|        play_count|\n",
      "+-------+--------------------+------------------+-------------------+------------------+\n",
      "|  count|               20988|             20988|              20988|             20988|\n",
      "|   mean|2.454828790113059...|0.1631884886601868|0.23804078521059654| 0.300695635601296|\n",
      "| stddev|1.426268588886973...| 2.469136495308594|  2.128711398676191|2.0760004697342644|\n",
      "|    min|                   0|               -10|                 -7|                -7|\n",
      "|    25%|       1219770712084|                -1|                 -1|                -1|\n",
      "|    50%|       2448131358742|                 0|                  0|                 0|\n",
      "|    75%|       3667902070805|                 1|                  1|                 1|\n",
      "|    max|       4964982194176|                23|                 17|                15|\n",
      "+-------+--------------------+------------------+-------------------+------------------+\n",
      "\n",
      "Data transformation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split, monotonically_increasing_id\n",
    "\n",
    "# Add a unique identifier column to the DataFrame\n",
    "test_data = test_data.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "# Split the 'features' column into separate columns for user_id, track_id, and play_count\n",
    "test_data = test_data.withColumn(\"user_id\", split(col(\"features\")[0][0].cast(\"string\"), \" \")[0].cast(\"int\"))\n",
    "test_data = test_data.withColumn(\"track_id\", split(col(\"features\")[0][1].cast(\"string\"), \" \")[0].cast(\"int\"))\n",
    "test_data = test_data.withColumn(\"play_count\", split(col(\"features\")[0][2].cast(\"string\"), \" \")[0].cast(\"int\"))\n",
    "\n",
    "# Drop rows with null values\n",
    "test_data = test_data.dropna()\n",
    "\n",
    "# Show summary of cleaned data\n",
    "test_data.summary().show()\n",
    "\n",
    "# Indicate when data transformation is done\n",
    "print(\"Data transformation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "994e1780-8c8b-470e-bc1c-d0e368702103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_id: string (nullable = true)\n",
      " |-- user_id: double (nullable = true)\n",
      " |-- track_id: double (nullable = true)\n",
      " |-- play_count: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|              row_id|            user_id|           track_id|         play_count|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|663cfc46eff9941de...| 3.6756520370513175| 3.1075330221641035|  2.849467203982779|\n",
      "|663cfc47eff9941de...|-0.6841017950883698|0.07530321083923087| 0.7578103674944087|\n",
      "|663cfc47eff9941de...|-1.2782707210287152| -1.404545350553354|-0.9536318207970136|\n",
      "|663cfc47eff9941de...|-2.7510797544587415|-1.2052667907857624|-0.8085763432086499|\n",
      "|663cfc47eff9941de...| -1.501083218520508|-1.3162265608271722| -1.176454029434011|\n",
      "|663cfc48eff9941de...| 1.2266289928350496|  3.156820016138062| 3.7090223564897933|\n",
      "|663cfc48eff9941de...| -2.031567618177572|-0.3587792300901715| 0.6069387849694297|\n",
      "|663cfc48eff9941de...| -1.707836872826653| -1.230572212985931| -1.309623153998263|\n",
      "|663cfc48eff9941de...|  1.307634124710383| 1.7670893955220124|  1.068281944658892|\n",
      "|663cfc49eff9941de...|  2.020557111559502|  3.790770968128053|  4.381158322343134|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Split 'features' array into separate columns\n",
    "split_features = training_data.select(\n",
    "    col(\"_id.oid\").alias(\"row_id\"),\n",
    "    col(\"features\").getItem(0).getItem(0).alias(\"user_id\"),\n",
    "    col(\"features\").getItem(0).getItem(1).alias(\"track_id\"),\n",
    "    col(\"features\").getItem(0).getItem(2).alias(\"play_count\")\n",
    ")\n",
    "\n",
    "# Show the schema and a sample of the split data\n",
    "split_features.printSchema()\n",
    "split_features.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d18c5a0a-cd03-4c31-9a84-9be2c781d2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_id: string (nullable = true)\n",
      " |-- user_id: double (nullable = true)\n",
      " |-- track_id: double (nullable = true)\n",
      " |-- play_count: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|              row_id|            user_id|           track_id|         play_count|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|663cfc46eff9941de...| 3.6756520370513175| 3.1075330221641035|  2.849467203982779|\n",
      "|663cfc47eff9941de...|-0.6841017950883698|0.07530321083923087| 0.7578103674944087|\n",
      "|663cfc47eff9941de...|-1.2782707210287152| -1.404545350553354|-0.9536318207970136|\n",
      "|663cfc47eff9941de...|-2.7510797544587415|-1.2052667907857624|-0.8085763432086499|\n",
      "|663cfc47eff9941de...| -1.501083218520508|-1.3162265608271722| -1.176454029434011|\n",
      "|663cfc48eff9941de...| 1.2266289928350496|  3.156820016138062| 3.7090223564897933|\n",
      "|663cfc48eff9941de...| -2.031567618177572|-0.3587792300901715| 0.6069387849694297|\n",
      "|663cfc48eff9941de...| -1.707836872826653| -1.230572212985931| -1.309623153998263|\n",
      "|663cfc48eff9941de...|  1.307634124710383| 1.7670893955220124|  1.068281944658892|\n",
      "|663cfc49eff9941de...|  2.020557111559502|  3.790770968128053|  4.381158322343134|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Split 'features' array into separate columns\n",
    "split_features01 = test_data.select(\n",
    "    col(\"_id.oid\").alias(\"row_id\"),\n",
    "    col(\"features\").getItem(0).getItem(0).alias(\"user_id\"),\n",
    "    col(\"features\").getItem(0).getItem(1).alias(\"track_id\"),\n",
    "    col(\"features\").getItem(0).getItem(2).alias(\"play_count\")\n",
    ")\n",
    "\n",
    "# Show the schema and a sample of the split data\n",
    "split_features.printSchema()\n",
    "split_features.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01703302-1869-4869-ad14-20e9f8c34620",
   "metadata": {},
   "outputs": [],
   "source": [
    "###IGNORE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import col, split, monotonically_increasing_id\n",
    "\n",
    "# Split 'features' array into separate columns\n",
    "split_features = training_data.select(\n",
    "    col(\"_id\").alias(\"row_id\"),  # Assuming \"_id\" is the correct column name\n",
    "    col(\"features\")[0][0].alias(\"user_id\"),\n",
    "    col(\"features\")[0][1].alias(\"track_id\"),\n",
    "    col(\"features\")[0][2].alias(\"play_count\")\n",
    ")\n",
    "\n",
    "# Split 'features' array into separate columns\n",
    "split_features01 = test_data.select(\n",
    "    col(\"_id\").alias(\"row_id\"),  # Assuming \"_id\" is the correct column name\n",
    "    col(\"features\")[0][0].alias(\"user_id\"),\n",
    "    col(\"features\")[0][1].alias(\"track_id\"),\n",
    "    col(\"features\")[0][2].alias(\"play_count\")\n",
    ")\n",
    "\n",
    "# Add a unique identifier column to the DataFrame\n",
    "training_data = training_data.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "# Split the 'features' column into separate columns for user_id, track_id, and play_count\n",
    "training_data = training_data.withColumn(\"user_id\", split(col(\"features\")[0][0].cast(\"string\"), \" \")[0].cast(\"int\"))\n",
    "training_data = training_data.withColumn(\"track_id\", split(col(\"features\")[0][1].cast(\"string\"), \" \")[0].cast(\"int\"))\n",
    "training_data = training_data.withColumn(\"play_count\", split(col(\"features\")[0][2].cast(\"string\"), \" \")[0].cast(\"int\"))\n",
    "\n",
    "# Drop rows with null values\n",
    "training_data = training_data.dropna()\n",
    "\n",
    "# Add a unique identifier column to the DataFrame\n",
    "test_data = test_data.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "# Split the 'features' column into separate columns for user_id, track_id, and play_count\n",
    "test_data = test_data.withColumn(\"user_id\", split(col(\"features\")[0][0].cast(\"string\"), \" \")[0].cast(\"int\"))\n",
    "test_data = test_data.withColumn(\"track_id\", split(col(\"features\")[0][1].cast(\"string\"), \" \")[0].cast(\"int\"))\n",
    "test_data = test_data.withColumn(\"play_count\", split(col(\"features\")[0][2].cast(\"string\"), \" \")[0].cast(\"int\"))\n",
    "\n",
    "# Drop rows with null values\n",
    "test_data = test_data.dropna()\n",
    "\n",
    "# Split 'features' array into separate columns\n",
    "split_features = training_data.select(\n",
    "    col(\"_id.oid\").alias(\"row_id\"),\n",
    "    col(\"features\").getItem(0).getItem(0).alias(\"user_id\"),\n",
    "    col(\"features\").getItem(0).getItem(1).alias(\"track_id\"),\n",
    "    col(\"features\").getItem(0).getItem(2).alias(\"play_count\")\n",
    ")\n",
    "\n",
    "# Split 'features' array into separate columns\n",
    "split_features01 = test_data.select(\n",
    "    col(\"_id.oid\").alias(\"row_id\"),\n",
    "    col(\"features\").getItem(0).getItem(0).alias(\"user_id\"),\n",
    "    col(\"features\").getItem(0).getItem(1).alias(\"track_id\"),\n",
    "    col(\"features\").getItem(0).getItem(2).alias(\"play_count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93b960ea-bf92-48ce-9b61-6cf3638691d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_id: string (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- track_id: double (nullable = true)\n",
      " |-- play_count: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter out null or NaN values from the play_count column\n",
    "split_features = split_features.filter(~col(\"play_count\").isNull())\n",
    "\n",
    "# Convert user_id column to integers\n",
    "split_features = split_features.withColumn(\"user_id\", col(\"user_id\").cast(\"int\"))\n",
    "\n",
    "# Verify the schema after conversion and filtering\n",
    "split_features.printSchema()\n",
    "\n",
    "# Now, proceed to train the ALS model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d47a5260-e16c-45d2-9dea-deaf3b887401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_id: string (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- track_id: double (nullable = true)\n",
      " |-- play_count: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter out null or NaN values from the play_count column\n",
    "split_features01 = split_features01.filter(~col(\"play_count\").isNull())\n",
    "\n",
    "# Convert user_id column to integers\n",
    "split_features01 = split_features01.withColumn(\"user_id\", col(\"user_id\").cast(\"int\"))\n",
    "\n",
    "# Verify the schema after conversion and filtering\n",
    "split_features01.printSchema()\n",
    "\n",
    "# Now, proceed to train the ALS model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f96504af-da3e-4b5d-a142-25acb2a47071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaned \n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have a DataFrame named 'data' containing your input data\n",
    "# Load your data into a DataFrame\n",
    "# data = spark.read.csv(\"your_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Convert 'track_id' column to integer type and filter out invalid values\n",
    "cleaned_data = split_features.withColumn(\"track_id\", col(\"track_id\").cast(\"integer\")) \\\n",
    "                   .filter(col(\"track_id\").isNotNull())\n",
    "print(\"Data Cleaned \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52da7811-379f-4193-acd2-1ae1f34402c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaned\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have a DataFrame named 'data' containing your input data\n",
    "# Load your data into a DataFrame\n",
    "# data = spark.read.csv(\"your_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Convert 'track_id' column to integer type and filter out invalid values\n",
    "cleaned_data01 = split_features01.withColumn(\"track_id\", col(\"track_id\").cast(\"integer\")) \\\n",
    "                   .filter(col(\"track_id\").isNotNull())\n",
    "print(\"Data Cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccfb3cf5-ab03-4863-a0fa-9c5feefb22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assuming your DataFrame is called split_features\n",
    "# Convert the track_id column to integer\n",
    "split_features = split_features.withColumn(\"track_id\", col(\"track_id\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2852443-4777-4ccb-be51-d4c24cbcf76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assuming your DataFrame is called split_features\n",
    "# Convert the track_id column to integer\n",
    "split_features01 = split_features01.withColumn(\"track_id\", col(\"track_id\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1a26563-3fb3-4a6f-8ad3-d962f3ef9321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the ALS model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/12 15:58:52 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/05/12 15:58:52 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assuming your DataFrame is called split_features\n",
    "# Convert the track_id column to integer\n",
    "split_features = split_features.withColumn(\"track_id\", col(\"track_id\").cast(\"integer\"))\n",
    "\n",
    "# Create an ALS instance\n",
    "als = ALS(maxIter=10, regParam=0.01, userCol=\"user_id\", itemCol=\"track_id\", ratingCol=\"play_count\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "\n",
    "# Train the ALS model\n",
    "print(\"Training the ALS model...\")\n",
    "model = als.fit(split_features)\n",
    "print(\"Model training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5e1d5b7-48ef-493d-806e-a5208f28fe1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:=====================================================>(578 + 1) / 579]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = model.transform(split_features)\n",
    "\n",
    "# Evaluate the model using RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"play_count\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {:.2f}\".format(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b84d5a9d-c4a7-4ed3-b0cf-b9580defab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:====================================================>(578 + 1) / 579]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) =  0.8799627835479484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "predictions = model.transform(split_features01)\n",
    "\n",
    "# Evaluate the model using an appropriate evaluation metric\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"play_count\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) = \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5252dd6-f7fc-4b5c-a928-1df2b2233490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and partitioned successfully.\n",
      "Training the ALS model with hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/12 20:20:52 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/05/12 20:20:52 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9425:===================================================>(576 + 3) / 579]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 0.8741945157100438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "split_features = split_features.repartition(\"user_id\")\n",
    "print(\"Data loaded and partitioned successfully.\")\n",
    "\n",
    "# Define ALS model\n",
    "als = ALS(userCol=\"user_id\", itemCol=\"track_id\", ratingCol=\"play_count\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [10, 20, 30]) \\\n",
    "    .addGrid(als.maxIter, [5, 10, 15]) \\\n",
    "    .addGrid(als.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"play_count\",\n",
    "                                predictionCol=\"prediction\")\n",
    "\n",
    "# Cache the data for reuse\n",
    "split_features.cache()\n",
    "\n",
    "# Define cross-validator\n",
    "cross_val = CrossValidator(estimator=als,\n",
    "                           estimatorParamMaps=param_grid,\n",
    "                           evaluator=evaluator,\n",
    "                           numFolds=3)\n",
    "\n",
    "# Train ALS model using cross-validation\n",
    "print(\"Training the ALS model with hyperparameter tuning...\")\n",
    "cv_model = cross_val.fit(split_features)\n",
    "print(\"Model training completed.\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = cv_model.transform(split_features01)\n",
    "\n",
    "# Evaluate the model using RMSE\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f58f635a-f843-4ce9-8361-c7b0324904e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "Rank: 20\n",
      "Max Iterations: 15\n",
      "Regularization Parameter: 0.01\n",
      "Accuracy of the best model: 0.8836422781420403\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Optionally, you can also print the best parameters found during hyperparameter tuning\n",
    "print(\"Best parameters:\")\n",
    "print(\"Rank:\", best_model.rank)\n",
    "print(\"Max Iterations:\", best_model._java_obj.parent().getMaxIter())\n",
    "print(\"Regularization Parameter:\", best_model._java_obj.parent().getRegParam())\n",
    "\n",
    "predictions = best_model.transform(split_features)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy of the best model:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51e1d408-e38c-4b77-b7d6-392c815c74f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the ALS model with hyperparameter tuning...\n",
      "Model training completed.\n",
      "Root Mean Squared Error (RMSE): 0.883230456181312\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Assuming split_features is your DataFrame containing user-item interactions\n",
    "split_features02 = split_features.repartition(\"user_id\")\n",
    "\n",
    "# Define ALS model\n",
    "als = ALS(userCol=\"user_id\", itemCol=\"track_id\", ratingCol=\"play_count\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid02 = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [5, 10, 15]) \\\n",
    "    .addGrid(als.maxIter, [5, 10, 15]) \\\n",
    "    .addGrid(als.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(als.alpha, [0.01, 0.1, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Define evaluator\n",
    "evaluator02 = RegressionEvaluator(metricName=\"rmse\", labelCol=\"play_count\",\n",
    "                                  predictionCol=\"prediction\")\n",
    "\n",
    "# Cache the data for reuse\n",
    "split_features02.cache()\n",
    "\n",
    "# Define cross-validator\n",
    "cross_val02 = CrossValidator(estimator=als,\n",
    "                             estimatorParamMaps=param_grid02,\n",
    "                             evaluator=evaluator02,\n",
    "                             numFolds=3)\n",
    "\n",
    "# Train ALS model using cross-validation\n",
    "print(\"Training the ALS model with hyperparameter tuning...\")\n",
    "cv_model02 = cross_val02.fit(split_features02)\n",
    "print(\"Model training completed.\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions02 = cv_model02.transform(split_features02)\n",
    "\n",
    "# Evaluate the model using RMSE\n",
    "rmse02 = evaluator02.evaluate(predictions02)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59e78041-9350-42e9-bca1-295c86ebfb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "Rank: 10\n",
      "Max Iterations: 10\n",
      "Regularization Parameter: 0.01\n",
      "Alpha: 0.01\n",
      "Accuracy of the best model: 0.883230456181312\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Get best model from cross-validation\n",
    "best_model = cv_model02.bestModel\n",
    "\n",
    "# Optionally, you can also print the best parameters found during hyperparameter tuning\n",
    "print(\"Best parameters:\")\n",
    "print(\"Rank:\", best_model.rank)\n",
    "print(\"Max Iterations:\", best_model._java_obj.parent().getMaxIter())\n",
    "print(\"Regularization Parameter:\", best_model._java_obj.parent().getRegParam())\n",
    "print(\"Alpha:\", best_model._java_obj.parent().getAlpha())\n",
    "\n",
    "# Calculate accuracy of the best model\n",
    "predictions = best_model.transform(split_features02)\n",
    "accuracy = evaluator02.evaluate(predictions)\n",
    "print(\"Accuracy of the best model:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a64ab-a3f7-4872-8b22-db9ce693d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Function to extract audio features from MP3 files\n",
    "def extract_features(file_path):\n",
    "    try:\n",
    "        # Load audio file\n",
    "        y, sr = librosa.load(file_path)\n",
    "        # Extract features (MFCC, spectral centroid, zero-crossing rate)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(y)\n",
    "        # Concatenate features into a single array\n",
    "        features = np.concatenate((mfcc, spectral_centroid, zero_crossing_rate), axis=0)\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to preprocess features (standardization)\n",
    "def preprocess_features(features):\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    features_standardized = scaler.fit_transform(features.T).T\n",
    "    return features_standardized\n",
    "\n",
    "# Function to apply dimensionality reduction (PCA)\n",
    "def apply_pca(features, n_components=None):\n",
    "    if n_components is None:\n",
    "        n_components = min(features.shape)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    features_pca = pca.fit_transform(features.T).T\n",
    "    return features_pca\n",
    "\n",
    "# Function to connect to MongoDB and insert data\n",
    "def insert_to_mongodb(features):\n",
    "    try:\n",
    "        # Connect to MongoDB\n",
    "        client = MongoClient('mongodb://127.0.0.1:27017/')\n",
    "        db = client['music_recommendation']\n",
    "        collection = db['audio_features']\n",
    "        # Insert features into MongoDB\n",
    "        collection.insert_one({\"features\": features.tolist()})\n",
    "        print(\"Data inserted into MongoDB successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting data into MongoDB: {e}\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Directory containing the MP3 files\n",
    "    dataset_path = \"/media/hdoop/C6760CE6760CD8D7/BDA PROJECT/fma_large\"  # Update this path\n",
    "    # Iterate through each folder (000 to 155)\n",
    "    for folder_index in range(156):  # Loop from 0 to 155\n",
    "        # Format the folder index to have leading zeros\n",
    "        folder_name = f\"{folder_index:03d}\"\n",
    "        folder_path = os.path.join(dataset_path, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(f\"Processing files in folder {folder_name}...\")\n",
    "            # Iterate through each MP3 file in the folder\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if file_name.endswith(\".mp3\"):\n",
    "                    file_path = os.path.join(folder_path, file_name)\n",
    "                    # Extract features from the MP3 file\n",
    "                    features = extract_features(file_path)\n",
    "                    if features is not None:\n",
    "                        # Preprocess features (standardization)\n",
    "                        features_standardized = preprocess_features(features)\n",
    "                        # Apply dimensionality reduction (PCA)\n",
    "                        features_pca = apply_pca(features_standardized)\n",
    "                        # Insert features into MongoDB\n",
    "                        insert_to_mongodb(features_pca)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def get_actual_folders(dataset_path):\n",
    "    actual_folders = set()\n",
    "    for folder_name in os.listdir(dataset_path):\n",
    "        if folder_name.isdigit():\n",
    "            actual_folders.add(int(folder_name))\n",
    "    return actual_folders\n",
    "\n",
    "def cleanup_mongodb(dataset_path):\n",
    "    try:\n",
    "        # Connect to MongoDB\n",
    "        client = MongoClient('mongodb://localhost:27017/')\n",
    "        db = client['music_recommendation']\n",
    "        collection = db['audio_features']\n",
    "        \n",
    "        # Get actual folders present in the dataset directory\n",
    "        actual_folders = get_actual_folders(dataset_path)\n",
    "        \n",
    "        # Define the range of expected folders\n",
    "        expected_folders = set(range(156))  # Assuming folders are numbered from 0 to 155\n",
    "        \n",
    "        # Find the intersection of actual and expected folders\n",
    "        folders_to_cleanup = actual_folders - expected_folders\n",
    "        \n",
    "        # Query MongoDB to find documents with folder numbers beyond the expected range\n",
    "        # Assuming folder numbers are stored in a field named \"folder_number\"\n",
    "        query = {\"folder_number\": {\"$in\": list(folders_to_cleanup)}}\n",
    "        unwanted_documents = collection.find(query)\n",
    "        \n",
    "        # Delete unwanted documents\n",
    "        for doc in unwanted_documents:\n",
    "            collection.delete_one({\"_id\": doc[\"_id\"]})\n",
    "        \n",
    "        print(\"Data cleanup completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning up MongoDB data: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Directory containing the MP3 files\n",
    "    dataset_path = \"D:/BDA PROJECT/fma_large\"\n",
    "    cleanup_mongodb(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613f9bc-2119-45d2-b52a-4697a7bf5d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "\n",
    "# Set up Kafka producer\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "\n",
    "# Set up Kafka consumer\n",
    "consumer = KafkaConsumer('user_activity_topic', bootstrap_servers='localhost:9092')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979563c9-a63f-4b5d-96ec-902f596df394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "# Create Kafka admin client\n",
    "admin_client = KafkaAdminClient(bootstrap_servers=\"localhost:9092\")\n",
    "\n",
    "# Define topics\n",
    "topics = [NewTopic(name=\"user_activity_topic\", num_partitions=1, replication_factor=1),\n",
    "          NewTopic(name=\"music_recommendation_topic\", num_partitions=1, replication_factor=1)]\n",
    "\n",
    "# Create topics\n",
    "admin_client.create_topics(new_topics=topics, validate_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2ef5e-fd30-49be-8813-67e4231f6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Kafka consumer for user activity data\n",
    "consumer = KafkaConsumer('user_activity_topic', bootstrap_servers='localhost:9092')\n",
    "\n",
    "# Set up Kafka producer for music recommendations\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "\n",
    "# Process user activity data and generate recommendations\n",
    "for message in consumer:\n",
    "    user_activity_data = message.value\n",
    "    # Use ALS model to generate music recommendations\n",
    "    recommendations = generate_recommendations(user_activity_data)\n",
    "    # Send recommendations to Kafka topic\n",
    "    producer.send('music_recommendation_topic', value=recommendations.encode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb6e897-a4d2-40da-a7df-f3be3883cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask example\n",
    "from flask import Flask, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Route to display music recommendations\n",
    "@app.route('/recommendations')\n",
    "def display_recommendations():\n",
    "    # Consume recommendations from Kafka topic\n",
    "    consumer = KafkaConsumer('music_recommendation_topic', bootstrap_servers='localhost:9092')\n",
    "    recommendations = []\n",
    "    for message in consumer:\n",
    "        recommendations.append(message.value.decode())\n",
    "    # Render recommendations on web interface\n",
    "    return render_template('recommendations.html', recommendations=recommendations)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
